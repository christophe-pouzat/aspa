# -*- ispell-local-dictionary: "american" -*-
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t
#+TITLE: /Analyse des séquences de potentiels d'action/ Tutorial
#+AUTHOR: Christophe Pouzat
#+EMAIL: christophe.pouzat@parisdescartes.fr
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.0)
#+LaTeX_CLASS: koma-article
#+LaTeX_CLASS_OPTIONS: [koma,11pt]
#+LaTeX_HEADER: \usepackage{cmbright}
#+LaTeX_HEADER: \usepackage[round]{natbib}
#+LaTeX_HEADER: \usepackage{alltt}
#+LaTeX_HEADER: \usepackage[usenames,dvipsnames]{xcolor}
#+LaTeX_HEADER: \renewenvironment{verbatim}{\begin{alltt} \scriptsize \color{Bittersweet} \vspace{0.2cm} }{\vspace{0.2cm} \end{alltt} \normalsize \color{black}}
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \lstloadlanguages{C,Gnuplot,bash,sh,R}
#+LaTeX_HEADER: \hypersetup{colorlinks=true,pagebackref=true}
#+STARTUP: indent
#+PROPERTY: header-args :eval no-export

#+NAME: org-latex-set-up
#+BEGIN_SRC emacs-lisp :exports none :results silent
(setq smartparens-mode nil)
(require 'ox-latex)
(setq org-export-latex-listings t)
(setq org-latex-listings 'listings)
(setq org-latex-listings-options
        '(("frame" "lines")
          ("basicstyle" "\\footnotesize")
          ("numbers" "left")
          ("numberstyle" "\\tiny")))
(add-to-list 'org-latex-classes
          '("koma-article"
             "\\documentclass{scrartcl}"
             ("\\section{%s}" . "\\section*{%s}")
             ("\\subsection{%s}" . "\\subsection*{%s}")
             ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
             ("\\paragraph{%s}" . "\\paragraph*{%s}")
             ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
(setq org-latex-pdf-process
      '("pdflatex -interaction nonstopmode -output-directory %o %f"
	"bibtex %b" 
	"pdflatex -interaction nonstopmode -output-directory %o %f" 
	"pdflatex -interaction nonstopmode -output-directory %o %f"))
#+END_SRC

#+NAME: set-gnuplot-pars
#+BEGIN_SRC gnuplot :session *gnuplot* :exports none :results silent :eval no-export
set terminal pngcairo size 1000,1000
#+END_SRC

#+NAME: stderr-redirection
#+BEGIN_SRC emacs-lisp :exports none
;; Redirect stderr output to stdout so that it gets printed correctly (found on
;; http://kitchingroup.cheme.cmu.edu/blog/2015/01/04/Redirecting-stderr-in-org-mode-shell-blocks/
(setq org-babel-default-header-args:sh
      '((:prologue . "exec 2>&1") (:epilogue . ":"))
      )
(setq org-babel-use-quick-and-dirty-noweb-expansion t)
#+END_SRC

#+RESULTS: stderr-redirection
: t

* The idea motivating this development
  :PROPERTIES:
  :CUSTOM_ID: the-idea-motivating-this-development
  :END:

The idea here is to implement the Unix/Linux "philosophy"--as exposed
for instance in the article of Arnold Robbins
[[http://www.linuxjournal.com/article/2762][/What's GNU/]]--to the
analysis of neuronal spike trains. Since spike trains make not too
voluminous data, they can be stored as text files (ASCII) and most
operations on them can be designed as "filters", that is programs
(usually but not always written in =C=) that read their input in text
format from the "standard input" (=stdin=) and send their result in text
format to the "standard output" (=stdout=). For graphical displays, we
are going to use [[http://gnuplot.info/][gnuplot]].

* Required software
  :PROPERTIES:
  :CUSTOM_ID: required-software
  :END:

The code will be written mostly in =C=. If you want a clear and quick
introduction to this language, check Ben Klemens:
[[http://modelingwithdata.org/about_the_book.html][Modeling With Data]].
To compile the code you will need a =C= compiler like
[[https://gcc.gnu.org/][gcc]]. If you are using =Linux= or =MacOS= it's
in a package from your favorite distribution, if you are using =Windows=
you will have to install [[https://cygwin.com/index.html][Cygwin]]. The
heavy computational work is going to be performed mainly by the
[[http://www.gnu.org/software/gsl/][gsl]] (the /GNU Scientific Library/)
that is easily installed through your package manager (from now on, for
windows users, the "package manager" refers to the one of =Cygwin=). The
graphs will be generated with [[http://www.gnuplot.info/][gnuplot]].
Windows user who want to use the interactive plotting capabilities of
the library (recommended) will also have to install
[[http://x.cygwin.com/][=Cygwin/X=]].

For now the compilation requires either =make= or [[http://scons.org/][Scons]].

** Getting and compiling the code
   :PROPERTIES:
   :CUSTOM_ID: getting-and-compiling-the-code
   :END:

The code is hosted on
[[https://github.com/christophe-pouzat/aspa][=GitHub=]]. The easiest is
to clone or download the repository (there is a button for that on the
GitHub page). Once you have the repository on your hard drive, go to the
code sub-directory and, if using =make=, type:

#+BEGIN_SRC sh :exports both :results output
cd ../code && make all
#+END_SRC

#+RESULTS:
#+begin_example
cc `pkg-config --cflags gsl` -g -Wall -O0 -std=gnu11    -c -o aspa_single.o aspa_single.c
ar cr libaspa.a aspa_single.o
cc `pkg-config --cflags gsl` -g -Wall -O0 -std=gnu11    -c -o aspa_read_spike_train.o aspa_read_spike_train.c
cc aspa_read_spike_train.o libaspa.a `pkg-config --libs gsl `  -o aspa_read_spike_train
cc `pkg-config --cflags gsl` -g -Wall -O0 -std=gnu11    -c -o aspa_mst_fns.o aspa_mst_fns.c
cc aspa_mst_fns.o libaspa.a `pkg-config --libs gsl `  -o aspa_mst_fns
cc `pkg-config --cflags gsl` -g -Wall -O0 -std=gnu11    -c -o aspa_mst_aggregate.o aspa_mst_aggregate.c
cc aspa_mst_aggregate.o libaspa.a `pkg-config --libs gsl `  -o aspa_mst_aggregate
cc `pkg-config --cflags gsl` -g -Wall -O0 -std=gnu11    -c -o aspa_mst_plot.o aspa_mst_plot.c
cc aspa_mst_plot.o libaspa.a `pkg-config --libs gsl `  -o aspa_mst_plot
#+end_example

or with =SCons=:

#+BEGIN_SRC sh :exports both :results output
scons -Q
#+END_SRC 

This will compile the library =libaspa.a= as well as a bunch of user
programs all starting with =aspa_=, like =aspa_read_spike_train=. As
mentioned previously, you need the =gsl= to be installed in order to
compile the code.

Once the compilation is done you should move the user programs to one of
the directories listed on your =PATH=, that is on one of the directories
appearing when you type:

#+BEGIN_SRC sh :exports both :results output
echo $PATH
#+END_SRC

#+RESULTS:
: /opt/jython/bin/:/usr/local/sbin:/usr/local/bin:/usr/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl

After that, you're in business.

* Data used
  :PROPERTIES:
  :CUSTOM_ID: data-used
  :END:

We are going to use spike trains obtained from the antennal lobe--the first
olfactory relay--of locusts, /Schistocerca americana/. These spike trains
can be found on the
[[https://christophe-pouzat.github.io/zenodo-locust-datasets-analysis/][zenodo-locust-datasets-analysis]]
GitHub repository. You can also find there a complete description of the
sorting procedure used to go from the raw data, that are available on
[[https://zenodo.org/record/21589][zenodo]], to the spike trains. We
will mostly use the spike trains from experiment =locust20010214= that
can be found at the following address:
[[https://github.com/christophe-pouzat/zenodo-locust-datasets-analysis/tree/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains]].

** Getting a spike train
   :PROPERTIES:
   :CUSTOM_ID: getting-a-spike-train
   :END:

We will start by downloading the spike train from unit 1 from =group=
=Spontaneous_1=. This is done by typing in the shell (I'm using the
"line continuation character, " to fit my lines on a single page of the
=PDF= version of this document, when typing directly to the shell you
don't need these line breaks):

#+NAME: download-u1-data-from-spont1-locust20010214
#+BEGIN_SRC sh :exports both :results output
wget https://raw.githubusercontent.com/christophe-pouzat/\
zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/\
locust20010214/locust20010214_spike_trains/\
locust20010214_Spontaneous_1_tetB_u1.txt
#+END_SRC

#+RESULTS:
#+begin_example
--2017-10-23 21:14:47--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u1.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 27743 (27K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u1.txt »

     0K .......... .......... .......                         100%  957K=0,03s

2017-10-23 21:14:47 (957 KB/s) — « locust20010214_Spontaneous_1_tetB_u1.txt » sauvegardé [27743/27743]

#+end_example

This "spike train" contains in fact the result of 30 consecutive
continuous acquisitions, each 29 s long with a 1 s gap in between, as is
made clear in the
[[https://christophe-pouzat.github.io/zenodo-locust-datasets-analysis/Locust_Analysis_with_R/locust20010214/Sorting_20010214_tetB.html][detailed
sorting description]] of this data set.

* Preliminary analysis
  :PROPERTIES:
  :CUSTOM_ID: preliminary-analysis
  :END:

** Reading the data
   :PROPERTIES:
   :CUSTOM_ID: reading-the-data
   :END:

In is not expected that the data (spike trains) one wants to work with
will be obtained in any standard format. That means that a usually
slightly "painful" work will be required (but that's always the case
when dealing with actual data) to read the data and reformat them in the
text (or binary) format used by =aspa=. Looking at the source code of
=aspa_read_spike_train= is the way to proceed (more specifically, look
at the code of =aspa_raw_fscanf= that is called by
=aspa_read_spike_train= and that is found in =aspa_single.c=).

The data we just downloaded are collections of spike times in "sample
times"--the time unit is therefore not the second but 1/15000
second--with one spike time per line. This can be seen by calling first
the =head= function (showing by default the first ten lines of the
file):

#+BEGIN_SRC sh :exports both :results output
head locust20010214_Spontaneous_1_tetB_u1.txt
#+END_SRC

#+RESULTS:
#+begin_example
4364.629
49876.8
50529.95
50988.26
51371.66
51769.29
52703.77
54772.34
56472.7
71766.51
#+end_example

Calling =tail= shows the last lines of the file (by default the last ten
lines):

#+BEGIN_SRC sh :exports both :results output
tail locust20010214_Spontaneous_1_tetB_u1.txt
#+END_SRC

#+RESULTS:
#+begin_example
13442792
13455679
13458610
13460049
13460517
13461154
13464139
13470059
13471539
13472243
#+end_example


Function =aspa_read_spike_train= will read these times from the =stdin=
and output them in a "nice" format (still a text file by default) to the
=stdout=. You can get a description to arguments accepted by the
function by calling it with the =--help= argument:

#+BEGIN_SRC sh :exports both :results output
./aspa_read_spike_train --help
#+END_SRC

#+RESULTS:
#+begin_example
Usage: 
  --in_bin: specify binary data input
  --out_bin: specify binary data output
  --sample2second <positive real>: the factor by which times
  in input data are divided in order get spike times in seconds
  used only when reading 'raw' data (default 15000)
  --inter_trial_interval <positive real>: the inter trial
  interval (in s) used only when reading 'raw' data
  --trial_duration <positive real>: the recorded duration
  (in s) of each trial used only when reading 'raw' data
  --stim_onset <real>: the stimulus onset time
  (in s) if that makes sense, used only when reading 'raw' data
  --stim_offset <real>: the stimulus offset time
  (in s) if that makes sense, used only when reading 'raw' data

#+end_example


For demonstration we can call it on the data file we just downloaded
(=locust20010214_Spontaneous_1_tetB_u1.txt=), writing the result into a
new text file =locust20010214_Spontaneous_1_tetB_u1.aspa= for further
inspection:

#+BEGIN_SRC sh :exports both :results output
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
locust20010214_Spontaneous_1_tetB_u1.txt > \
locust20010214_Spontaneous_1_tetB_u1.aspa
#+END_SRC

#+RESULTS:

We can then look at the first 25 lines of our new file with:

#+BEGIN_SRC sh :exports both :results output
head -n 25 locust20010214_Spontaneous_1_tetB_u1.aspa 
#+END_SRC

#+RESULTS:
#+begin_example
# Number of trials: 28
# Number of aggregated trials: 1
# Stimulus onset: 0 (s)
# Stimulus offset: 0 (s)
# Single trial duration: 29 (s)


# Start of trial: 0
# Trial start time: 0 (s)
# Number of spikes: 94
0.290975
3.32512
3.36866
3.39922
3.42478
3.45129
3.51358
3.65149
3.76485
4.78443
5.06381
5.11507
5.24077
5.28448
5.31933
#+end_example


We see that the "non-data" element are on lines starting with a "#"
character. The "head" of the file specifies how many trial are in the
file and gives some other information. The data from trial 0 (we start
counting at 0) com next after two blank lines. To see the whole file
interactively you can type:

#+BEGIN_SRC sh :eval never
less locust20010214_Spontaneous_1_tetB_u1.aspa 
#+END_SRC

** Basic statistics
   :PROPERTIES:
   :CUSTOM_ID: basic-statistics
   :END:

Program =aspa_mst_fns= (=mst= stands for "multiple spike trains" and
=fns= for "[[https://en.wikipedia.org/wiki/Five-number_summary][Five-number summary]]") return elementary statics related to a spike train data set.
A description of its use is obtained by calling the program with the =--help= argument:

#+NAME: aspa_mst_fns-help
#+BEGIN_SRC sh :exports both :results output
./aspa_mst_fns --help
#+END_SRC

#+RESULTS: aspa_mst_fns-help
: Usage: 
:   --in_bin: specify binary data input
: 
: Returns five number summary and additional stats.


We can call this function directly on the output of =aspa_read_spike_train= using a [[http://www.linfo.org/pipe.html][pipe]] with:

#+NAME: aspa_read_spike_train-example-1
#+BEGIN_SRC sh :exports both :results output
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
locust20010214_Spontaneous_1_tetB_u1.txt | \
./aspa_mst_fns
#+END_SRC

#+RESULTS: aspa_read_spike_train-example-1
#+begin_example
Data from 28 trials.
The mean rate is: 4.10222 Hz.
The inter spike interval statistics are:
  The sample contains 3303 elements.
  The mean and SD are   : 0.2333 and 0.4660.
  The median and MAD are: 0.0546 and 0.0359.
The five number summary:
  Min.   1st qrt Median 3rd qrt Max. 
  0.0157 0.0369  0.0546 0.1491  4.5264
A 95% confidence interval for the lag 1 Spearman rank correlation is: [0.400336,0.443483].
#+end_example


** Basic plots
   :PROPERTIES:
   :CUSTOM_ID: basic-plots
   :END:

There are several plots one might want to create at an early stage of spike train data analysis. Since most of these plots are more "attractive" when built from data with a response to a stimulus, we will start by getting one such case (from the same experiment and same neuron):

#+NAME: download-locust20010214_C3H_1_tetB_u1.txt
#+BEGIN_SRC sh :exports both :results output
wget https://raw.githubusercontent.com/christophe-pouzat/\
zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/\
locust20010214/locust20010214_spike_trains/\
locust20010214_C3H_1_tetB_u1.txt
#+END_SRC

#+RESULTS: download-locust20010214_C3H_1_tetB_u1.txt
#+begin_example
--2017-10-23 20:53:00--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_C3H_1_tetB_u1.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 29318 (29K) [text/plain]
Sauvegarde en : « locust20010214_C3H_1_tetB_u1.txt.1 »

     0K .......... .......... ........                        100% 1,75M=0,02s

2017-10-23 20:53:01 (1,75 MB/s) — « locust20010214_C3H_1_tetB_u1.txt.1 » sauvegardé [29318/29318]

#+end_example
  
This file contains the responses to 25 stimulations with =cis-3-hexen-1-ol=. The classical way of displaying such data is the =raster plot=. This plot as well as several over ones we will shortly see is generated by calling =aspa_mst_plot=. As usual, calling the function with argument =--help= gives us a basic explanation on how to use it:

#+NAME: aspa_mst_plot_help
#+BEGIN_SRC sh :results output :exports both 
./aspa_mst_plot --help
#+END_SRC

#+RESULTS: aspa_mst_plot_help
#+begin_example
Usage: 
  --in_bin: specify binary data input
  --text: specify text output
  --what <string>: one of 'raster', 'cp_rt', 'cp_wt',
  'cp_norm', 'lrank', the type of plot (see bellow)
  --lag <positive integer>: the lag used in lagged
    ranked plots (default at 1).

An interactive plot is generated.
If what is set to 'raster' a raster plot is generated.
If what is set to 'cp_rt' the observed counting process
in 'real' time is generated, that is trial appear one after
the other.
If what is set to 'cp_wt' the observed counting processes
corresponding to each trial are shown on the 'within trial time'.
If what is set to 'cp_norm' the normalized aggregated counting
process is displayed (normalization means here that the step size
due to each spike in each trial is 1/number of trials; in a sense
the 'mean' counting process is displayed).
If what is set to 'lrank', isi are ranked from the smallest to
the largest and the rank of isi i+lag is plotted against the
lag of isi i.
#+end_example


*** Raster plot

Here, to get the classical raster we do:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=raster
#+END_SRC

This will make a new window appear with a plot similar to the one we will now construct after calling the function with an additional argument (you can type =q= to kill the plot window):

#+BEGIN_SRC sh 
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=raster --text > \
locust20010214_C3H_1_tetB_u1.raster
#+END_SRC

#+RESULTS:

Here instead of the "new window output" we generated at text output (that's what the =--text= argument means) sent to the =stdout= and redirected this =stdout= to a file called =locust20010214_C3H_1_tetB_u1.raster=. We can now build "by hand" with =gnuplot= the same figure as the one we directly got (we have now more control on the output):

#+HEADERS: :file fig/locust20010214_C3H_1_tetB_u1_raster.png  
#+BEGIN_SRC gnuplot :exports both :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Time (s)'
set ylabel 'Trial'
plot [0:30] [0:26] 'locust20010214_C3H_1_tetB_u1.raster' \
     index 0 using 1:2 with filledcurve closed lc 'grey',\
     '' index 1:25 using 1:2 with dots lc 'black'
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_C3H_1_tetB_u1_raster.png]]

*** A fancy trick

We can also make the raster plot and get the basic stats printed at once with the [[https://www.gnu.org/software/coreutils/manual/html_node/tee-invocation.html][tee]] command as follows:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
locust20010214_C3H_1_tetB_u1.txt | tee >(aspa_mst_plot --what=raster) | \
./aspa_mst_fns
#+END_SRC

*** Counting process plots

There are several ways to create a "counting process" plot. The first one, used mainly for checking data stationarity is building the "true" observed counting process plot, that is at each spike time the step function jumps by one unit and successive trials are shown one after the other as they /actually/ occurred. This is what is specified with argument =cp_rt= to option =what=:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_rt
#+END_SRC

Giving a plot looking like:

#+BEGIN_SRC sh :exports none
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_rt --text > \
locust20010214_C3H_1_tetB_u1.cp_rt
#+END_SRC

#+RESULTS:

#+HEADERS: :file fig/locust20010214_C3H_1_tetB_u1_cp_rt.png  
#+BEGIN_SRC gnuplot :exports results :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Time (s)'
set title 'Observed counting process'
set ylabel 'Events count'
plot 'locust20010214_C3H_1_tetB_u1.cp_rt' using 1:2 with steps lc 'black'
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_C3H_1_tetB_u1_cp_rt.png]]

We might also want to look at the individual observed counting processes after realigning them on the stimulus onset. This is obtained with argument =cp_wt= to option =what=:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_wt
#+END_SRC

resulting in a plot looking like:

#+BEGIN_SRC sh :exports none
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_wt --text > \
locust20010214_C3H_1_tetB_u1.cp_wt
#+END_SRC

#+RESULTS:

#+HEADERS: :file fig/locust20010214_C3H_1_tetB_u1_cp_wt.png  
#+BEGIN_SRC gnuplot :exports results :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Time (s)'
set title 'Observed counting processes'
set ylabel 'Events count'
plot [0:29] [0:245] 'locust20010214_C3H_1_tetB_u1.cp_wt' index 0 using 1:2 with filledcurve closed lc 'grey',\
     '' index 1:25 using 1:2 with steps lc 'black'
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_C3H_1_tetB_u1_cp_wt.png]]


We can also decide that to see if there is a response or not, we can construct the average step function. That is, we replace the step size in the previous plot by 1/N (N is the number of trials) and we sum all these resulting step functions. This is done with argument =cp_norm= to option =what=:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_norm
#+END_SRC

resulting in a plot looking like:

#+BEGIN_SRC sh :exports none
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt | \
./aspa_mst_plot --what=cp_norm --text > \
locust20010214_C3H_1_tetB_u1.cp_norm
#+END_SRC

#+RESULTS:

#+HEADERS: :file fig/locust20010214_C3H_1_tetB_u1_cp_norm.png  
#+BEGIN_SRC gnuplot :exports results :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Time (s)'
set title 'Observed mean counting process'
set ylabel 'Mean events count'
plot [0:29] [0:145] 'locust20010214_C3H_1_tetB_u1.cp_norm' index 0 using 1:2 with filledcurve closed lc 'grey',\
     '' index 1 using 1:2 with steps lc 'black'
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_C3H_1_tetB_u1_cp_norm.png]]

One might want to prepare a more sophisticated plot for, say, a publication showing both the individual observed counting processes and their average. This could be done as follows, by creating first some files with the "properly" formated data:

#+BEGIN_SRC sh :exports both :results output
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 \
--stim_onset=10 --stim_offset=11 < \
locust20010214_C3H_1_tetB_u1.txt > \
locust20010214_C3H_1_tetB_u1.aspa

./aspa_mst_plot --what=cp_norm --text < \
locust20010214_C3H_1_tetB_u1.aspa > \
locust20010214_C3H_1_tetB_u1.cp_norm

./aspa_mst_plot --what=cp_wt --text < \
locust20010214_C3H_1_tetB_u1.aspa > \
locust20010214_C3H_1_tetB_u1.cp_wt
#+END_SRC

#+RESULTS:

Then within =gnuplot=:

#+HEADERS: :file fig/locust20010214_C3H_1_tetB_u1_cp_norm_wt.png  
#+BEGIN_SRC gnuplot :exports both :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Time (s)'
set title 'Observed counting processes'
set ylabel 'Events count'
plot [0:29] [0:245] 'locust20010214_C3H_1_tetB_u1.cp_wt' \
     index 0 using 1:2 with filledcurve closed lc 'grey',\
     '' index 1:25 using 1:2 with steps lc 'black',\
     'locust20010214_C3H_1_tetB_u1.cp_norm' index 1 \
     using 1:2 with steps lc 'red' linewidth 2
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_C3H_1_tetB_u1_cp_norm_wt.png]]

*** Lagged ranked ISI plot

Coming back to the spontaneous data, a good graphical way to look for correlations between successive inter spike intervals is to rank them (from the smallest to the largest) before plotting the rank of interval i+lag against the rank of interval i. We can do that with the spontaneous data as follows:

#+BEGIN_SRC sh :eval never
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 --stim_onset=10 --stim_offset=11 < \ locust20010214_Spontaneous_1_tetB_u1.txt | ./aspa_mst_plot --what=lrank
#+END_SRC

We then obtain a graph looking like:

#+BEGIN_SRC sh :exports none
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 --stim_onset=10 --stim_offset=11 < \
			locust20010214_Spontaneous_1_tetB_u1.txt | \
    ./aspa_mst_plot --what=lrank --text > \
		    locust20010214_Spontaneous_1_tetB_u1.lrank
#+END_SRC

#+RESULTS:

#+HEADERS: :file fig/locust20010214_Spontaneous_1_tetB_u1_lrank.png  
#+BEGIN_SRC gnuplot :exports results :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Rank of ISI i'
set title 'Lagged ranked ISIs'
set ylabel 'Rank of ISI i+1'
plot 'locust20010214_Spontaneous_1_tetB_u1.lrank' using 1:2 with dots
#+END_SRC

#+RESULTS:
[[file:fig/locust20010214_Spontaneous_1_tetB_u1_lrank.png]]
** Histograms
A cross-validation based binwidth selection is implemented in =aspa=. It follows Mats Rudemo "Empirical Choice of Histograms and Kernel Density Estimators" (/Scandinavian Journal of Statistics/ *9*:65-78, 1982) approach. In short given a sample of inter spike intervals (ISI), a cross validation score approximating the integrated mean squared error (between the estimated density and the true one) is computed for a sequence of binwidths (or equivalently number of bins). This curve score vs number of bins can be plotted and the the "best" choice is the number of bins minimizing it. An example follows in the next sub-section.

*** Getting the cross validation score   
We first get the ISI sample using function =aspa_mst_isi= whose description is:

#+NAME: aspa_mst_isi-help
#+BEGIN_SRC sh :exports both :results output
./aspa_mst_isi --help
#+END_SRC

#+RESULTS: aspa_mst_isi-help
: usage: ./aspa_mst_isi [-b --bin] [-h --help]
: 
:   -b --bin: the data read from the 'stdin' are in binary format.
:   -h --help: prints this message.
:  The program reads data from the 'stdin' (default in text format)
:  most likely resulting from a call to 'aspa_read_spike_train',
:  gets the inter spike intervals and writes the number of ISIs
:  followed by the individual ISIs (one per line) to the stdout
: 

We use it as follows, storing the obtained ISI sample in a file named =u1isi1.txt=:

#+NAME: aspa_mst_isi-use
#+BEGIN_SRC sh :exports both :results output
./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
			locust20010214_Spontaneous_1_tetB_u1.txt | \
    ./aspa_mst_isi > u1isi1.txt
#+END_SRC

#+RESULTS: aspa_mst_isi-use

We can check the head of the generated file with =head=:

#+NAME: u1isi1.txt-head
#+BEGIN_SRC sh :exports both :results output
head u1isi1.txt
#+END_SRC

#+RESULTS: u1isi1.txt-head
#+begin_example
3303
3.03414
0.04354
0.03056
0.0255599
0.02651
0.0622902
0.13791
0.11336
1.01958
#+end_example

We use next function =aspa_hist_bw= whose description is:

#+NAME: aspa_hist_bw-help
#+BEGIN_SRC sh :exports both :results output
./aspa_hist_bw --help
#+END_SRC

#+RESULTS: aspa_hist_bw-help
#+begin_example
usage: ./aspa_hist_bw [-l --log] [-f --from=integer]
          [-t --to=integer] [-b --best] [-h --help]

  -l --log: should the log of the observations be used?
  -f --from <positive integer>: the smallest number of bins to explore
     (default set to 2).
  -t --to <positive integer>: the largest number of bins to explore
     (default set to the number of observations divided by 5).
  -b --best: should only the best number of bins be printed to the 'stdout'?
  -h --help: prints this message.
 The program reads data from the 'stdin' (in text format),
 the first line should contain the number of observations (integer)
 the following lines should contain the observations, one per line
 in decimal notation. If a log transformed of the data is requested
 it is applied first. Then a sequence of histograms with number of
 bins variying between from and to (inclusive) is contructed. For each
 histogram, the empirical bin probability (bin count divided by sample
 size), p_hat_i for each bin i is obtained and the score is computed as
 follows: (2-(n+1) * sum(p_hat_i^2))*m/(n-1) where m is the number of
 bins and where the summation is done over the bins. The best number
 of bins is the one giving the smallest score.
 The program prints to the 'stdout' the number of bins and the
 corresponding cross-validation scores on two columns (default)
  or only the best number of bins if 'best' is used as an argument.

#+end_example

On the ISI sample we just got that gives (using the log of the observations to limit the consequences of the skewness of the distribution):

#+NAME: aspa_hist_bw-with-u1isi1.txt
#+BEGIN_SRC sh :exports both :results output
./aspa_hist_bw --log < u1isi1.txt > u1bw1.txt
#+END_SRC

#+RESULTS: aspa_hist_bw-with-u1isi1.txt
: Sample size: 3303
: Exploring now number of bins between 2 and 330.
: Using a log transformation of the data.
: Histograms will be built between -4.15413 and 1.50993.
: The best number of bins is: 46 giving a score of -2.07509

Using =gnuplot= we quickly visualize the curve:

#+NAME: u1bw1-fig
#+HEADERS: :file fig/locust20010214_Spontaneous_1_tetB_u1_bw.png  
#+BEGIN_SRC gnuplot :exports both :session *gnuplot* :eval no-export
set grid
unset key
set xlabel 'Number of bins'
set ylabel 'Cross validation score'
plot 'u1bw1.txt' using 1:2 with lines lc 'black'\
     linewidth 2
#+END_SRC

#+RESULTS: u1bw1-fig
[[file:fig/locust20010214_Spontaneous_1_tetB_u1_bw.png]]

*** Building the histogram

Once the "best" number of bins is known (or at least one has decided on the number of bins to use), we call =aspa_hist= to construct the histogram. Its description is:

#+NAME: aspa_hist-help
#+BEGIN_SRC sh :exports both :results output
./aspa_hist --help
#+END_SRC

#+RESULTS: aspa_hist-help
#+begin_example
usage: ./aspa_hist -n --n_bins=integer [-l --log] 
          [-p --prob] [-h --help]

  -n --n_bins <positive integer>: the number of bins to use.
  -l --log: should the log of the observations be used?
  -p --prob: should the result be normalized so that the
     the histogram integral is one?
  -h --help: prints this message.
 The program reads data from the 'stdin' (in text format),
 the first line should contain the number of observations (integer)
 the following lines should contain the observations, one per line
 in decimal notation. If a log transformed of the data is requested
 it is applied first. Then a histogram with 'n_bins' bins is contructed.
 If a log transformation was used, the bins boundaries are transformed
 back to the original scale. If argument 'prob' is specified, the histogram
 is normalized such that its integral is one (it becomes of proper PDF
 estimator), otherwise the bin counts are kept.
 The program prints to the 'stdout' on 3 columns: the left bin boundary;
 the right bin boundary; the bin count or bin frequency (depending on the
 specification of argument 'prob').

#+end_example

Using the previous ISI sample and the best number of bins we construct the histogram and save it in file =u1hist1.txt= with:

#+NAME: aspa_hist-use
#+BEGIN_SRC sh :exports both :results output
./aspa_hist -n 46 -p --log < u1isi1.txt > u1hist1.txt
#+END_SRC

#+RESULTS: aspa_hist-use
: Sample size: 3303
: Using a log transformation of the data.

The plot of the histogram with =gnuplot= is then done as follows:

#+NAME: u1hist1-fig
#+HEADERS: :file fig/locust20010214_Spontaneous_1_tetB_u1_isi_hist.png  
#+BEGIN_SRC gnuplot :exports both :session *gnuplot* :eval no-export
set grid
unset key
set title 'ISI density estimate neuron 1 spont. 1'
set xlabel 'Inter spike interval (s)'
set ylabel 'Estimated density (1/s)'
plot [0:0.5] [] 'u1hist1.txt' using 1:3 with steps lc 'black'\
     linewidth 2
#+END_SRC

#+RESULTS: u1hist1-fig
[[file:fig/locust20010214_Spontaneous_1_tetB_u1_isi_hist.png]]

** ISI histograms of the seven "good" neuron of dataset =locust20010214=

We can proceed and compute the ISI histograms of the 6 other good neurons of the experiment. We start by downloading the data with:

#+NAME: download-u2-to-u7-data-from-spont1-locust20010214
#+BEGIN_SRC sh :exports code :results output
prefix=https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis
prefix="$prefix"/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/
prefix="$prefix"locust20010214_Spontaneous_1_tetB_u
for i in {2..7} ; do
    wget "$prefix"$i".txt"
done
#+END_SRC

#+RESULTS: download-u2-to-u7-data-from-spont1-locust20010214
#+begin_example
--2017-10-25 17:42:17--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u2.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 30090 (29K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u2.txt »

     0K .......... .......... .........                       100% 10,9M=0,003s

2017-10-25 17:42:17 (10,9 MB/s) — « locust20010214_Spontaneous_1_tetB_u2.txt » sauvegardé [30090/30090]

--2017-10-25 17:42:18--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u3.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 11487 (11K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u3.txt »

     0K .......... .                                          100% 17,5M=0,001s

2017-10-25 17:42:18 (17,5 MB/s) — « locust20010214_Spontaneous_1_tetB_u3.txt » sauvegardé [11487/11487]

--2017-10-25 17:42:18--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u4.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 16047 (16K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u4.txt »

     0K .......... .....                                      100% 10,5M=0,001s

2017-10-25 17:42:18 (10,5 MB/s) — « locust20010214_Spontaneous_1_tetB_u4.txt » sauvegardé [16047/16047]

--2017-10-25 17:42:18--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u5.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 41264 (40K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u5.txt »

     0K .......... .......... .......... ..........           100% 9,17M=0,004s

2017-10-25 17:42:18 (9,17 MB/s) — « locust20010214_Spontaneous_1_tetB_u5.txt » sauvegardé [41264/41264]

--2017-10-25 17:42:18--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u6.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 7828 (7,6K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u6.txt »

     0K .......                                               100% 25,5M=0s

2017-10-25 17:42:19 (25,5 MB/s) — « locust20010214_Spontaneous_1_tetB_u6.txt » sauvegardé [7828/7828]

--2017-10-25 17:42:19--  https://raw.githubusercontent.com/christophe-pouzat/zenodo-locust-datasets-analysis/master/Locust_Analysis_with_R/locust20010214/locust20010214_spike_trains/locust20010214_Spontaneous_1_tetB_u7.txt
Certificat de l'autorité de certification « /etc/ssl/certs/ca-certificates.crt » chargé
Résolution de raw.githubusercontent.com… 151.101.92.133
Connexion à raw.githubusercontent.com|151.101.92.133|:443… connecté.
requête HTTP transmise, en attente de la réponse… 200 OK
Taille : 34966 (34K) [text/plain]
Sauvegarde en : « locust20010214_Spontaneous_1_tetB_u7.txt »

     0K .......... .......... .......... ....                 100% 10,4M=0,003s

2017-10-25 17:42:19 (10,4 MB/s) — « locust20010214_Spontaneous_1_tetB_u7.txt » sauvegardé [34966/34966]

#+end_example

We now get the ISIs stored in files names =uXisi1.txt= where =X= takes successively values 2 to 7:

#+NAME: get-ISI-spontaneous1-u2-to-u7
#+BEGIN_SRC sh :exports both :results output
file_name=locust20010214_Spontaneous_1_tetB_u
for i in {2..7} ; do
    ./aspa_read_spike_train --inter_trial_interval=30 --trial_duration=29 < \
			"$file_name"$i".txt" | \
	./aspa_mst_isi > "u"$i"isi1.txt"
done
#+END_SRC

#+RESULTS: get-ISI-spontaneous1-u2-to-u7

We check if some ISI values are null:

#+NAME: check-null-isi-spontaneous1-u1-to-u7
#+BEGIN_SRC sh :exports both :results output
for i in {1..7} ; do
    in="u"$i"isi1.txt"
    sed -n '2,$p' < $in > dump
    echo "Unit" $i "has" $(grep "0$" < dump | wc -l) \
	 "ISIs equal to zero among" $(($(wc -l < $in) - 1))
done
#+END_SRC

#+RESULTS: check-null-isi-spontaneous1-u1-to-u7
: Unit 1 has 0 ISIs equal to zero among 3303
: Unit 2 has 0 ISIs equal to zero among 3574
: Unit 3 has 0 ISIs equal to zero among 1339
: Unit 4 has 0 ISIs equal to zero among 1890
: Unit 5 has 4 ISIs equal to zero among 4912
: Unit 6 has 0 ISIs equal to zero among 909
: Unit 7 has 1 ISIs equal to zero among 4155
 
Since these null ISIs are always making less than a thousands of the sample size, we remove them. We just have to be careful since the first line of the ISI files contains the number of events. It should therefore be changed if we remove lines:

#+NAME: remove-null-isi-from-u1-and-u7
#+BEGIN_SRC sh :exports both :results output
for i in {1..7} ; do
    # in contains input file name
    in="u"$i"isi1.txt"
    # remove first line of in and save result in dump
    sed -n '2,$p' < $in > dump
    # check how many lines with just 0 dump contains
    zero=$(grep "0$" < dump | wc -l)
    if [[ $zero != 0 ]] ; then
       # if there are lines with just zero remove them
       sed -i "/0$/d" $in
       # compute the new number of events in two steps
       original=$(sed -n '1p' < $in)
       new=$(($original - $zero))
       # update the number of events
       sed -i "1s/.*/$new/" $in
    fi
done
#+END_SRC

#+RESULTS: remove-null-isi-from-u1-and-u7

We can then construct the ISI histograms (output not shown):
 
#+NAME: get-ISI-hist-spontaneous1-u1-to-u7  
#+BEGIN_SRC sh :exports code :results output
for i in {1..7} ; do
    in="u"$i"isi1.txt"
    out="u"$i"_isi_dens1.txt"
    n=$(./aspa_hist_bw --best --log < $in)
    ./aspa_hist --n_bins=$n --log --prob < $in > $out
done
#+END_SRC

#+RESULTS: get-ISI-hist-spontaneous1-u1-to-u7
#+begin_example
Sample size: 3303
Exploring now number of bins between 2 and 330.
Using a log transformation of the data.
Histograms will be built between -4.15413 and 1.50993.
The best number of bins is: 46 giving a score of -2.07509
Sample size: 3303
Using a log transformation of the data.
Sample size: 3574
Exploring now number of bins between 2 and 357.
Using a log transformation of the data.
Histograms will be built between -5.36657 and 1.34706.
The best number of bins is: 57 giving a score of -2.32044
Sample size: 3574
Using a log transformation of the data.
Sample size: 1339
Exploring now number of bins between 2 and 133.
Using a log transformation of the data.
Histograms will be built between -5.80906 and 1.99318.
The best number of bins is: 26 giving a score of -1.81599
Sample size: 1339
Using a log transformation of the data.
Sample size: 1890
Exploring now number of bins between 2 and 189.
Using a log transformation of the data.
Histograms will be built between -5.95235 and 1.70152.
The best number of bins is: 28 giving a score of -1.77396
Sample size: 1890
Using a log transformation of the data.
Sample size: 4908
Exploring now number of bins between 2 and 490.
Using a log transformation of the data.
Histograms will be built between -9.20898 and 0.848055.
The best number of bins is: 75 giving a score of -3.35658
Sample size: 4908
Using a log transformation of the data.
Sample size: 909
Exploring now number of bins between 2 and 90.
Using a log transformation of the data.
Histograms will be built between -4.3505 and 2.09433.
The best number of bins is: 12 giving a score of -1.35576
Sample size: 909
Using a log transformation of the data.
Sample size: 4154
Exploring now number of bins between 2 and 415.
Using a log transformation of the data.
Histograms will be built between -6.43758 and 1.06502.
The best number of bins is: 74 giving a score of -2.41961
Sample size: 4154
Using a log transformation of the data.
#+end_example


We can now make a graph with all the estimated ISI densities as follows:

#+NAME: u1-to-7-isi-dens1-fig
#+HEADERS: :file fig/locust20010214_Spontaneous_1_tetB_u1_7_isi_dens.png  
#+BEGIN_SRC gnuplot :exports both :session *gnuplot* :eval no-export
set grid
set title 'ISI density estimate for neurons 1 to 7 spont. 1'
set xlabel 'Inter spike interval (s)'
set ylabel 'Estimated density (1/s)'
plot [0:0.5] [0:20] 'u1_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u2_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u3_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u4_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u5_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u6_isi_dens1.txt' using 1:3 with steps linewidth 2,\
     'u7_isi_dens1.txt' using 1:3 with steps linewidth 2
#+END_SRC

#+RESULTS: u1-to-7-isi-dens1-fig
[[file:fig/locust20010214_Spontaneous_1_tetB_u1_7_isi_dens.png]]
